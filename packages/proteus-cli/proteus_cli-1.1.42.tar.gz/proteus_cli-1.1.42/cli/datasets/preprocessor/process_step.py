import os
from collections import OrderedDict
from contextlib import contextmanager, ExitStack
from itertools import chain
from threading import RLock, Thread
from typing import Iterator, Union, Sequence

from cli.api.hooks import TqdmUpWithReport
from cli.datasets.preprocessor.config import StepConfigWithMetadata
from cli.datasets.sources.common import Source
from cli.datasets.sources.local import LocalSource
from cli.utils.files import upload_file, download_file, PathMeta, RequiredFilePath
from cli.utils.sync import TaskDependencySemaphore

from cli import proteus, config


def files_exist_in_bucket(outputs, bucket_url):
    for output in outputs:
        response = proteus.api.get(bucket_url, headers={}, stream=False, contains=output, retry=True)
        files = response.json().get("results")
        if len(files) == 0:
            return False

    return True


def process_step(
    progress: TqdmUpWithReport,
    step: StepConfigWithMetadata,
    input_source: Source,
    output_source: LocalSource,
    cases_url,
    base_output_source: LocalSource,
    allow_missing_files: Sequence[str] = tuple(),
):

    with ExitStack() as lock_input_files:
        # Download inputs in parallel
        files = OrderedDict()

        def _download_input_file(input_file):
            found_input = lock_input_files.enter_context(
                download_input_file(input_file, input_source, output_source, step.keep, progress, step.step_name)
            )
            return found_input.download_name or input_file, found_input

        for download_name, found_input in proteus.bucket.each_item_parallel(
            len(step.input), step.input, each_item_fn=_download_input_file, workers=config.WORKERS_DOWNLOAD_COUNT
        ):
            if download_name in files:
                if not isinstance(files[download_name], list):
                    files[download_name] = [files[download_name]]

                files[download_name].append(found_input)
            else:
                files[download_name] = found_input

        input_files = tuple(chain(*(([x] if not isinstance(x, list) else x) for x in files.values())))

        # Preprocess inputs
        if step.preprocessing_fn:

            def download_func(dependency_file, dir_path):
                subpath = output_source.to_relative(dir_path)
                local_input_source = input_source.cd(subpath)
                local_output_source = output_source.cd(subpath)

                # Always keep dependency files outside the local input source. PE, the following
                # structure is very typical and the include folder is re-used many times
                #
                # /cases/testing/SIMULATION_1/file.DATA
                # /include/file.grid
                #
                # If the input_source is in /cases/testing/SIMULATION_1/, we would like for
                # /include/file.grid to not be removed because most likely it will be used
                # by /cases/testing/SIMULATION_2/, /cases/validation/SIMULATION_99/, etc
                dependency_uri = local_input_source.cd(input_source.dirname(dependency_file)).uri
                keep = not local_input_source.uri.startswith(dependency_uri) or step.keep

                return lock_input_files.enter_context(
                    download_input_file(
                        dependency_file,
                        local_input_source,
                        local_output_source,
                        keep,
                        progress,
                        step.step_name + ".dependency",
                    )
                ).full_path

            progress.set_description(step.step_name)
            step.preprocessing_fn(
                download_func=download_func,
                output_source=output_source,
                base_output_source=base_output_source,
                input_files=input_files,
                allow_missing_files=allow_missing_files,
                workers=config.WORKERS_DOWNLOAD_COUNT,
                progress=progress,
                proc_name=step.step_name,
                **{**files},
            )
            progress.set_description(step.step_name)

        # Find and upload outputs
        found_outputs = []
        for output in step.output:
            found_output_files = list(output_source.list_contents(output))
            if len(found_output_files) != 1 and isinstance(output, RequiredFilePath):
                raise FileNotFoundError(
                    f"Output {output} was suposed to be generated by {step.name} "
                    f"from {step.root}, but the file was not found."
                )

            for found_output in found_output_files:
                found_outputs.append(found_output)
                progress.set_postfix({"uploading": base_output_source.to_relative(found_output.path)})
                upload_file(base_output_source.to_relative(found_output.path), found_output.path, cases_url)
                progress.set_postfix({})
                progress.set_description(step.step_name)

            progress.update(1)
            progress.refresh()

    return step


@contextmanager
def download_input_file(
    input_file: Union[PathMeta, str],
    input_source: Source,
    output_source: LocalSource,
    keep: bool,
    progress: TqdmUpWithReport,
    step_info=None,
) -> Iterator[PathMeta]:

    if not isinstance(input_file, PathMeta):
        input_file = PathMeta(input_file)

    dir_output_file = os.path.join(output_source.uri, input_file)

    try:
        transformed_input, output_path, _ = download_file(input_file, dir_output_file, input_source, progress)
    except FileNotFoundError:
        transformed_input = None
        # Try to download the replacement if the file was not found
        try:
            if input_file.replace_with is not None:
                transformed_input = download_input_file(
                    input_file.replace_with, input_source, output_source, keep, progress, step_info.step_name
                )
        except FileNotFoundError:
            pass

        if transformed_input is None:
            raise

    if transformed_input is not None:
        transformed_input = input_file.clone(transformed_input)
        transformed_input.full_path = output_path
    else:
        transformed_input = input_file

    with _lock_dependency(output_path, keep, step_info):
        yield transformed_input


DOWNLOAD_INPUT_FILE_LOCK = RLock()
DOWNLOAD_INPUT_FILE_SEMAPHORES = {}


@contextmanager
def _lock_dependency(output_path, keep, step_info):
    """
    Ensures a file is not removed until all tasks depending on it are finished.
    """
    with DOWNLOAD_INPUT_FILE_LOCK:
        file_semaphore = DOWNLOAD_INPUT_FILE_SEMAPHORES.get(output_path)
        if not file_semaphore:
            file_semaphore = TaskDependencySemaphore(output_path)
            DOWNLOAD_INPUT_FILE_SEMAPHORES[output_path] = file_semaphore

    # A step using a file must prevent other steps from removing the file until
    # it has finished using the file.

    file_semaphore.acquire_task(step_info)
    yield
    with DOWNLOAD_INPUT_FILE_LOCK:
        file_semaphore.release_task(step_info)
        if file_semaphore.value == 0:
            DOWNLOAD_INPUT_FILE_SEMAPHORES.pop(output_path, None)

    if not keep:
        # If the task to remove files is run in a separated thread, and by chance the next step
        # is not using this task's step, the next step may continue.
        rm_thread = Thread(daemon=True, target=_rm_input, args=(output_path, file_semaphore, step_info))
        rm_thread.start()


def _rm_input(output_path, file_semaphore, step_info):
    file_semaphore.acquire_dependecy(step_info)
    if output_path:
        try:
            os.remove(output_path)
        except FileNotFoundError:
            pass
