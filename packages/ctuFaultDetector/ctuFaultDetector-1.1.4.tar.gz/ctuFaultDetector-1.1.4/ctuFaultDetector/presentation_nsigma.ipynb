{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir(r\"..\")\n",
    "from ctuFaultDetector.utils import *\n",
    "from ctuFaultDetector.visual import *\n",
    "from ctuFaultDetector.models.deviationClassifier import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we will need to prepair the data. We have a csv file with all the signals. The columns of the csv file are:\n",
    "\n",
    "idx, label, meas_id, Force_x, Force_y, Force_z, Torque_x, Torque_y, Torque_z\n",
    "\n",
    "- idx is the identifier of the signal\n",
    "- label is the boolean or None label of the signal\n",
    "- meas_id is the identifier of the group of measurement (e.g. day of the measurement) in the form of an int 1 - n\n",
    "- The other columns are signal feature columns\n",
    "\n",
    "\n",
    "We create variables:\n",
    "\n",
    "data, signals, labels, correct_signals, anom_signals\n",
    "\n",
    "- data: list of tuples in the form of (signal : pd.DataFrame, label : bool)\n",
    "- signals: list of signals in the form of pd.Dataframe\n",
    "- labels: list of boolean/None labels\n",
    "- correct_signals: list of signals with label \"True\"\n",
    "- wrong_signals: list of signals with label \"False\"\n",
    "- unlabeled_signals: list of signals with label \"None\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data.\n",
    "We will begin our presentation with loading the data from the dataset. Execute the following cell to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"./ctuFaultDetector/data/dataset.csv\", id = [1,2,5,6])\n",
    "signals = [i[0] for i in data]\n",
    "labels = [i[1] for i in data]\n",
    "correct_signals = [transform_pd_to_npy(i[0]) for i in data if i[1] == False]\n",
    "anom_signals = [transform_pd_to_npy(i[0]) for i in data if i[1]==True]\n",
    "print(\"Number of correct: \", len(correct_signals), \", Number of anomalous: \", len(anom_signals))\n",
    "print()\n",
    "print(\"Number of signals:\", len(data))\n",
    "print()\n",
    "print(\"Format of a signal:\\n\\n\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the data, let us see the methods we developed. Let's divide the dataset into a training and testing split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "training_set, testing_set = get_n_th_fold(data, 0, small_train=False)\n",
    "correct_training_signals = [transform_pd_to_npy(i[0]) for i in training_set if i[1] == False]\n",
    "anom_training_signals = [transform_pd_to_npy(i[0]) for i in training_set if i[1]==True]\n",
    "correct_testing_signals = [transform_pd_to_npy(i[0]) for i in testing_set if i[1] == False]\n",
    "anom_testing_signals = [transform_pd_to_npy(i[0]) for i in testing_set if i[1]==True]\n",
    "print(f\"The length of the training set: {len(training_set)}\")\n",
    "print(f\"Number of correct/anomalous processes: {len(correct_training_signals)}/{len(anom_training_signals)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"The length of the testing set: {len(testing_set)}\")\n",
    "print(f\"Number of correct/anomalous processes: {len(correct_testing_signals)}/{len(anom_testing_signals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we are all set up to start testing the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-$\\sigma$ classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up a deviation classifier for 6 dimensional signals with n = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk = deviationClassifier(6,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it with the prepared training dataset. We choose the accuracy criterion to optimize it for maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk.fit_whole_supervised_dataset(training_set, \"ACC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a basic evaluation of a signal from the testing dataset using a supervised n-$\\sigma$ classifier and its visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = np.random.randint(0, len(testing_set))\n",
    "signal, label = testing_set[random_sample]\n",
    "pred = dk.predict_full_signal(signal)\n",
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Real label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the whole testing set, this time without visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_offline_dk(classifier, testing_set):\n",
    "    correct_testing_signals = [transform_pd_to_npy(i[0]) for i in testing_set if i[1] == False]\n",
    "    anom_testing_signals = [transform_pd_to_npy(i[0]) for i in testing_set if i[1]==True]\n",
    "    correct_perf = 0\n",
    "    anomaly_perf = 0\n",
    "    for sig in correct_testing_signals:\n",
    "        correct_perf += not classifier.predict_full_signal(sig, vis = False)\n",
    "    for sig in anom_testing_signals:\n",
    "        anomaly_perf += classifier.predict_full_signal(sig, vis = False)\n",
    "    print(f\"Correctly predicted successfull signals: {correct_perf}/{len(correct_testing_signals)} --> TNR = {correct_perf/len(correct_testing_signals)}\")\n",
    "    print(f\"Correctly predicted anomalous signals: {anomaly_perf}/{len(anom_testing_signals)} --> TPR =  {anomaly_perf/len(anom_testing_signals)}\")\n",
    "    print(f\"Accuracy: {(correct_perf+anomaly_perf)/len(testing_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_offline_dk(testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the classifier to minimal TPR of 0.8 on the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk.fit_whole_supervised_dataset(training_set, \"TPR\", 0.8)\n",
    "print(\"\\n\\nTesting results:\")\n",
    "evaluate_offline_dk(dk, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the result for TNR higher than 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk.fit_whole_supervised_dataset(training_set, \"TNR\", 0.9)\n",
    "print(\"\\n\\nTesting results:\")\n",
    "evaluate_offline_dk(dk, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the unsupervised classifier performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the unsupervised deviation classifier with an unsupervised dataset and test the results. Let's remove the labels from the training dataset and fit it to the classifier. The evaluation will be done with labeled testing data from signals that were never seen by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_training_set = [(sig, None) for sig, label in training_set]\n",
    "print(unlabeled_training_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we removed the labeles.\n",
    "Now that we have the dataset without labels, we can train the classifier. Let's assume the fail rate of 19\\% (twice remove the worst 10\\% of the dataset) and train the detector.\n",
    "\n",
    "#### In dk.fit_whole_unsupervised_dataset:\n",
    "\n",
    "*You can adjust the \"criterion\" parameter to one from [\"TPR\", \"TNR\", \"ACC\", \"sACC\"] and play around with value parameter so see all possible tunings. \n",
    "\n",
    "*You can also change the \"success_ratio\" parameter to remove more or less samples.\n",
    "\n",
    "*Or adjust the n_of_filtration_steps to do the removal process multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk.fit_whole_unsupervised_dataset(unlabeled_training_set, success_ratio = 0.9, criterion = \"sACC\", n_of_filtration_steps = 2)\n",
    "print(\"\\n\\nTesting results:\")\n",
    "evaluate_offline_dk(dk, testing_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the online variation of this method, which is capable of evaluating partial signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online training takes more time than the offline training. Run the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dk.online_fit(training_set, criterion = \"sACC\", value = 3, print_progress = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a random sample from the testing dataset with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sample = np.random.randint(0, len(testing_set))\n",
    "signal, label = testing_set[random_sample]\n",
    "signal = transform_pd_to_npy(signal)\n",
    "pred = dk.predict_partial_signal(transform_pd_to_npy(signal[:700, :]))\n",
    "print(f\"Prediction: {pred}\")\n",
    "print(f\"Real label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the classifier, we will measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.linspace(0, 1000, 10)\n",
    "counter = 0\n",
    "positive = 0\n",
    "totally_wrong_anom_signals = 0\n",
    "for signal in correct_testing_signals:\n",
    "    for timestamp in timestamps:\n",
    "        counter += 1\n",
    "        positive += dk.predict_partial_signal(signal[:int(timestamp), :], vis = False)\n",
    "    TP_pos, TN_pos, FP_pos, FN_pos = 0, counter - positive, positive, 0\n",
    "    TP_anom, TN_anom, FP_anom, FN_anom = 0,0,0,0\n",
    "for signal in anom_testing_signals:\n",
    "    lastpred = False\n",
    "    for timestamp in timestamps:\n",
    "        counter += 1\n",
    "        prediction = dk.predict_partial_signal(signal[:int(timestamp), :], vis = False)\n",
    "        if not lastpred and not prediction:\n",
    "            if timestamp == timestamps[-1]:\n",
    "                TN_anom -= len(timestamps)\n",
    "                FN_anom += len(timestamps)\n",
    "                totally_wrong_anom_signals += 1\n",
    "            TN_anom += 1\n",
    "        elif (not lastpred and prediction) or (lastpred and prediction):\n",
    "            TP_anom += 1\n",
    "        elif lastpred and not prediction:\n",
    "            FN_anom += 1\n",
    "        lastpred = prediction or lastpred\n",
    "TP, TN, FP, FN = TP_pos + TP_anom, TN_pos + TN_anom, FP_pos + FP_anom, FN_pos + FN_anom\n",
    "tpr = TP/(TP+FN)\n",
    "fpr = 1-(TN/(FP+TN))\n",
    "print(f\"\\n\\n\\nTP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\\n\\n\")\n",
    "print(f\"Accuracy {(TP+TN)/(TP+TN+FP+FN)}\")\n",
    "print(f\"TPR: {tpr}, FPR: {fpr}\")\n",
    "print(f\"TP+FN: {TP+FN}, TN+FP: {TN+FP}\")\n",
    "print(f\"Poorly predicted anom signals as a whole: {totally_wrong_anom_signals}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
