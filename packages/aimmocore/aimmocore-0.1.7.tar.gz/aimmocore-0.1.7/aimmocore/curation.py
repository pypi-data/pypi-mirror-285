"""curation class """

import time
import asyncio
from bson import ObjectId
import requests
from loguru import logger
from aimmocore import config as conf
from aimmocore.core.database import MongoDB
from aimmocore.core import services as acs
from aimmocore.core import images as aci
from aimmocore.core.storages import StorageConfig, StorageType
from aimmocore.core import utils
from aimmocore.server.schemas.datasets import ProcessStatus
from aimmocore.core.event import SingletonEventLoop as sel


class Curation:
    def __init__(self, api_key: str):
        """initiailize curation

        Args:
            api_key (str): _description_
        """
        self.api_key = api_key
        self._valid_api_key()
        self.db = MongoDB()
        self.db.connect()
        self.event_loop = sel.get_instance().get_loop()

    def curate_dataset(self, name: str, storage_config: StorageConfig) -> str:
        """
        Synchronously curates a dataset by internally calling an asynchronous method using the event loop.

        Args:
            name (str): The name of the dataset to be curated.
            storage_config (StorageConfig): Configuration object containing settings and parameters
                                            necessary for storing and handling the dataset.
        """
        return self.event_loop.run_until_complete(self.curate_dataset_async(name, storage_config))

    async def curate_dataset_async(self, name: str, storage_config: StorageConfig) -> str:
        """
        Asynchronously curates a dataset by generating image URLs from a storage configuration,
        and processing these URLs to manage dataset curation.

        Args:
            name (str): The name of the dataset to be curated.
            storage_config (StorageConfig): The configuration object that includes details like storage type
                                            and credentials, which are essential for generating image URLs.

        Returns:
            str: A unique identifier for the curated dataset, typically a dataset ID generated by the system.
        """
        self._valid_api_key()
        image_urls = self.get_image_urls(storage_config)
        request_datas = self.prepare_request_data(image_urls)
        dataset_id = self.process_dataset(name, request_datas)
        self._request(dataset_id)
        self.post_process_dataset(dataset_id)
        logger.debug(f"Created dataset_id: {dataset_id}")
        return dataset_id

    def _request(self, dataset_id: str) -> str:
        """_summary_"""
        headers = {"Authorization": f"Bearer {self.api_key}"}
        data = {"dataset_id": dataset_id}
        file_path = dataset_id + ".txt"
        # Open the file in binary mode
        with open(f"{conf.AIMMOCRE_WORKDIR}/{dataset_id}.txt", "rb") as file:
            files = {"file": (file_path, file)}
            response = requests.post(conf.CURATION_UPLOAD_ENDPOINT, headers=headers, data=data, files=files, timeout=30)
        # Check the response status code
        if response.status_code == 200 and response.text:
            response_data = response.json()
            self.event_loop.run_until_complete(acs.update_dataset_curation_status(self.db, response_data))
            return dataset_id
        logger.error(response.text)

    def get_image_urls(self, storage_config: StorageConfig):
        """
        Generates and returns a list of image URLs based on the provided storage configuration.
        This function currently supports only Azure storage types.

        Args:
            storage_config (StorageConfig): A configuration object that includes the storage type
                                            and credentials necessary for accessing the storage service.

        Returns:
            list[str]: A list of image URLs generated from the storage configuration.

        Raises:
            NotImplementedError: If the storage type specified in the storage configuration is not supported.
                                Currently, only 'AZURE' is implemented.
        """
        if storage_config.storage_type != StorageType.AZURE:
            raise NotImplementedError(f"Image URL generation not implemented for {storage_config.storage_type}")
        return storage_config.generate_image_urls()

    def prepare_request_data(self, image_urls):
        """Prepare request data from image URLs."""
        return [
            {
                "id": utils.hash_filename(image_url.split("?")[0]),
                "model_id": "va-torch-meta-emd:2",
                "image_url": image_url,
                "model_type": ["meta", "emd"],
            }
            for image_url in image_urls
        ]

    async def _async_process_dataset(self, name, request_datas):
        """Asynchronous method to process dataset by inserting and writing to files."""
        dataset_id = str(ObjectId())
        file_path = f"{conf.AIMMOCRE_WORKDIR}/{dataset_id}.txt"
        tasks = [
            acs.insert_raw_files(self.db, request_datas),
            acs.insert_raw_dataset(self.db, name, dataset_id, request_datas),
            utils.write_to_file(file_path, request_datas),
        ]
        await asyncio.gather(*tasks)
        return dataset_id

    def process_dataset(self, name, request_datas):
        """Process dataset by inserting and writing to files."""
        event_loop = sel.get_instance().get_loop()
        dataset_id = event_loop.run_until_complete(self._async_process_dataset(name, request_datas))
        return dataset_id

    def post_process_dataset(self, dataset_id):
        """Handle post-processing tasks such as thumbnail generation."""
        self.event_loop.run_until_complete(aci.generate_thumbnail())

    def tracing_curation_task(self, dataset_id: str, tracing: bool = False) -> str:
        """
        Tracks and updates the curation task status for a given dataset ID.

        Args:
            dataset_id (str): The unique identifier for the dataset whose curation status is to be tracked.
            tracing (bool, optional): If True, continues to trace until completion or until retries are exhausted.
                                    Defaults to False.

        Raises:
            RuntimeError: If the tracing does not complete successfully after the allowed number of retries.

        Returns:
            str: The final status of the curation task.
        """
        logger.debug(f"Tracing curation, curation_id is {dataset_id}")
        url = f"{conf.CURATION_STATUS_ENDPOINT}?dataset_id={dataset_id}"

        while True:
            json_results = self._make_request(url)
            if not json_results:
                break
            status_str = json_results.get("status")

            logger.debug(f"Tracing curation task, dataset_id is {dataset_id}, status is {status_str}")

            try:
                status = ProcessStatus(status_str)
            except ValueError:
                logger.error(f"Invalid status '{status_str}' received for dataset {dataset_id}")
                raise

            if status in [ProcessStatus.COMPLETED, ProcessStatus.FAILED, ProcessStatus.ERROR]:
                if status == ProcessStatus.COMPLETED:
                    self._update_results(dataset_id, json_results)
                self._update_status(dataset_id, status.value)
                return status.value

            if not tracing:
                break
            time.sleep(5)  # Wait before retrying if tracing is enabled

    def _valid_api_key(self):
        """_summary_"""
        headers = {"Authorization": f"Bearer {self.api_key}"}
        response = requests.get(conf.CURATION_AUTH_ENDPOINT, headers=headers, timeout=30)
        if response.status_code != 200 and response.text:
            logger.error(response.text)
            raise RuntimeError("Invalid API Key")
        logger.debug("API Key is authorized")

    async def _update_results_async(self, dataset_id: str, results: dict):
        """
        Asynchronously update the curation results in the database.

        Args:
            dataset_id (str): The unique identifier for the dataset.
            results (dict): The results dictionary containing the curation outcomes,
                            specifically the 'emd_results' and the 'status' of the curation.
        """
        processed_count = len(results["emd_results"])
        meta_list = await acs.update_curation_results(self.db, dataset_id, results)
        await acs.update_dataset_info(self.db, dataset_id, results["status"], meta_list, processed_count)

    def _update_results(self, dataset_id: str, results: dict):
        """
        Update the curation results in the database by calling the asynchronous update method.


        Args:
            dataset_id (str): The unique identifier for the dataset.
            results (dict): The results dictionary containing the outcomes of the dataset curation.

        """
        # Schedule and run the asynchronous update operations as a single coroutine
        task = self.event_loop.create_task(self._update_results_async(dataset_id, results))
        self.event_loop.run_until_complete(task)

    def _update_status(self, dataset_id: str, status: str):
        """_summary_

        Args:
            dataset_id (str): _description_
            status (str): _description_
        """
        self.event_loop.run_until_complete(acs.update_dataset_info_status(self.db, dataset_id, status))

    def _make_request(self, url: str):
        headers = {"Authorization": f"Bearer {self.api_key}"}
        response = requests.get(url, headers=headers, timeout=30)
        if response.status_code == 200:
            return response.json()
        logger.info(f"Maybe creating collection...[{response.status_code}]")
        return {}
