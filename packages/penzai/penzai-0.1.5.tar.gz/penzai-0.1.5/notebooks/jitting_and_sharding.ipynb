{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DfejMHs4lr8V"
   },
   "source": [
    "*Copyright 2024 The Penzai Authors.*\n",
    "\n",
    "*Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at*\n",
    "\n",
    "> http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "*Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USGIPdLYDzSo"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-deepmind/penzai/blob/main/notebooks/jitting_and_sharding.ipynb) [![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/google-deepmind/penzai/blob/main/notebooks/jitting_and_sharding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qA3Q3cm5Cca9"
   },
   "source": [
    "# Jitting and Sharding Penzai Models\n",
    "\n",
    "Penzai is designed to be compatible with JAX's standard function transformations, including JIT-compilation and array sharding. If you're already familiar with JIT compilation and distributed arrays in JAX, you shouldn't have to learn anything fundamentally new to apply it to Penzai! But Penzai does provide some utilities to make it easier to construct and manipulate shardings for Penzai models.\n",
    "\n",
    "This notebook walks through some of the common aspects of JIT-compilation and sharding as they apply to Penzai tools and Penzai models. It assumes some basic familiarity with JAX's [JIT compilation](https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html) and [distributed array](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHr2rnIL8DzM"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkW4lYKAu-oR"
   },
   "source": [
    "Before we can get started in earnest, we need to set up the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozG8ERNavDos"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmxgAcFQmZkB"
   },
   "source": [
    "To run this notebook, you need a Python environment with `penzai` and its dependencies installed.\n",
    "\n",
    "In Colab or Kaggle, you can install it using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGZH58j8mPkj"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import penzai\n",
    "except ImportError:\n",
    "  !pip install penzai[notebook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iog3oMAMGCMG"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v26wYYx6QSn3"
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Mh2mAuiQ4aa"
   },
   "outputs": [],
   "source": [
    "import penzai\n",
    "from penzai import pz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQU6QfoZVYOL"
   },
   "outputs": [],
   "source": [
    "from penzai.example_models import gemma\n",
    "from penzai.example_models import simple_mlp\n",
    "from penzai.toolshed import basic_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGzhV5uWvkvB"
   },
   "source": [
    "### Setting up Penzai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjGkV8F8vmpi"
   },
   "source": [
    "For this tutorial, we'll enable Treescope (Penzai's pretty-printer) as the default IPython pretty-printer. This is recommended when using Penzai in an interactive environment. We'll also enable automatic array visualization, which also makes it easy to visualize array shardings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YodWk_jmva_7"
   },
   "outputs": [],
   "source": [
    "pz.ts.register_as_default()\n",
    "pz.ts.register_autovisualize_magic()\n",
    "pz.ts.register_context_manager_magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_0C5AtfEJeE"
   },
   "outputs": [],
   "source": [
    "pz.enable_interactive_context()\n",
    "pz.ts.active_autovisualizer.set_interactive(pz.ts.ArrayAutovisualizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jaslw09Fn6t"
   },
   "source": [
    "We'll assume this notebook is running on a backend with eight devices. If needed, you can force JAX to treat the CPU backend as multiple devices using\n",
    "```python\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCiROtE5Fq53"
   },
   "outputs": [],
   "source": [
    "pz.show(jax.local_devices())\n",
    "assert jax.local_device_count() == 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20jyxs7mEAJj"
   },
   "source": [
    "## JIT-Compiling Penzai Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUc2cj5KGeWx"
   },
   "source": [
    "By convention, Penzai models are always JAX PyTrees with only arraylike leaves. This means you can always JIT-compile a function that takes a Penzai model as input or returns one as output, using ordinary `jax.jit`.\n",
    "\n",
    "For instance, suppose we have the following Penzai model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFzZ1ilqHGX3"
   },
   "outputs": [],
   "source": [
    "mlp_def = simple_mlp.MLP.from_config([8, 32, 32, 8])\n",
    "mlp_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6La9rQwFHKZI"
   },
   "source": [
    "We could JIT-compile the initializer for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hszY05jnHUmp"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def init_my_model(mlp_def):\n",
    "  return pz.nn.initialize_parameters(mlp_def, jax.random.key(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OM8o-4h0HcFW"
   },
   "outputs": [],
   "source": [
    "mlp = init_my_model(mlp_def)\n",
    "mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7hjPmtvHfiN"
   },
   "source": [
    "And we can just as easily JIT-compile a loss function that uses it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svhSUwNhHjEl"
   },
   "outputs": [],
   "source": [
    "# Just for demonstration; a real loss function would probably be more complex\n",
    "# and involve a batch of examples\n",
    "\n",
    "@jax.jit\n",
    "def simple_mse_loss(mlp, inputs, target):\n",
    "  output = mlp(inputs)\n",
    "  diffs = (output - target).untag(\"features\").unwrap()\n",
    "  return jnp.sum(jnp.square(diffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JA5A3F3EH7aS"
   },
   "outputs": [],
   "source": [
    "simple_mse_loss(mlp, pz.nx.ones({\"features\": 8}), pz.nx.zeros({\"features\": 8}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPQ6YLaTIDXS"
   },
   "source": [
    "Note that Penzai models store their parameters inside the model, not in a separate parameter dictionary. This means you probably don't want to do this:\n",
    "```python\n",
    "# !!!! PROBABLY NOT WHAT YOU WANT TO DO:\n",
    "jitted_call = jax.jit(mlp)\n",
    "jitted_call(some_input)\n",
    "```\n",
    "The reason is that this will \"bake in\" the parameters of your MLP as constants in the compiled function, so JAX will need to recompile it if you update the parameters of the MLP.\n",
    "\n",
    "Instead, you can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6djxpFGIvEB"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def jitted_call(mlp, arg):\n",
    "  return mlp(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avCdQj1RIxxB"
   },
   "outputs": [],
   "source": [
    "jitted_call(mlp, pz.nx.ones({\"features\": 8}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHdvNVOQIy3h"
   },
   "source": [
    "To save you the trouble of doing this manually when you want to JIT your model's `__call__`, Penzai provides a wrapper that does this automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "opD4iPPWI9wp"
   },
   "outputs": [],
   "source": [
    "from penzai.toolshed import jit_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNqN1KcrI_K-"
   },
   "outputs": [],
   "source": [
    "jitted_mlp = jit_wrapper.Jitted(mlp)\n",
    "jitted_mlp(pz.nx.ones({\"features\": 8}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmdHDdskJFBD"
   },
   "source": [
    "`Jitted` is actually just an ordinary Penzai layer. It holds your model inside it as an attribute, and jit-compiles `__call__`:\n",
    "\n",
    "```python\n",
    "@pz.pytree_dataclass\n",
    "class Jitted(pz.Layer):\n",
    "  body: pz.LayerLike\n",
    "\n",
    "  def __call__(self, argument: Any, /) -> Any:\n",
    "    return jitted_call(self.body, argument)\n",
    "\n",
    "```\n",
    "You can see the model stored inside it as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7_e_55gJPnz"
   },
   "outputs": [],
   "source": [
    "jitted_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PnXazRBJQ6y"
   },
   "source": [
    "It defines `__call__` to be JIT-compiled, including itself as a non-static argument. This means that JAX will automatically re-used the cached compiled program if you call multiple `Jitted` layers with the same structure, even if you update the parameters.\n",
    "\n",
    "It will also re-compile if you make modifications. For instance, we can freely insert new logic into our \"jitted MLP\", and those new functions will run under JIT as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe1zUVgvJzqL"
   },
   "outputs": [],
   "source": [
    "@pz.pytree_dataclass\n",
    "class PrintMyValue(pz.Layer):\n",
    "  def __call__(self, arg):\n",
    "    pz.show(\"Intermediate:\", arg)\n",
    "    return arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LALsYXG7J7lN"
   },
   "outputs": [],
   "source": [
    "patched_jitted_mlp = (\n",
    "    pz.select(jitted_mlp)\n",
    "    .at_instances_of(pz.nn.Elementwise)\n",
    "    .insert_before(PrintMyValue())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ak8hKvyuKIMk"
   },
   "outputs": [],
   "source": [
    "patched_jitted_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0e3prcYFKF6k"
   },
   "outputs": [],
   "source": [
    "patched_jitted_mlp(pz.nx.ones({\"features\": 8}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJuu4bxwMdig"
   },
   "source": [
    "And you can always pull the model back out of the `Jitted` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QIHeKA7NJxR"
   },
   "outputs": [],
   "source": [
    "jitted_mlp.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsW8ckRkMgNl"
   },
   "outputs": [],
   "source": [
    "assert jitted_mlp.body is mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZZag_0nEOIB"
   },
   "source": [
    "## Sharding Basics, and Visualizing Shardings with Treescope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q18ckhsWMk7A"
   },
   "source": [
    "Penzai's array autovisualizer supports showing shardings and sharded arrays by default. This section explains the basics of JAX's distributed array shardings and how you can visualize the different components in Treescope. (See [this page](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) for the official documentation of JAX's sharding system.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMTVux7AQiTj"
   },
   "source": [
    "### Positional shardings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypez_YTWNMPp"
   },
   "source": [
    "At a high level, you can think of a \"sharding\" as a multidimensional array of device objects, which will be matched with your multidimensional array of data to determine which part of the array ends up on each device. You generally build a sharding by starting with a NumPy array of devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB7ywoGRNL1e"
   },
   "outputs": [],
   "source": [
    "from jax.experimental import mesh_utils\n",
    "devices = mesh_utils.create_device_mesh((8,))\n",
    "devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0BXLtlANv2O"
   },
   "source": [
    "A simple type of sharding is `PositionalSharding`, which essentially just holds onto these devices and tracks some extra JAX-specific information. If you print out a `PositionalSharding` in Treescope, it color-codes the devices and shows you their arrangement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3MesweYOqK2"
   },
   "outputs": [],
   "source": [
    "pos_sharding = jax.sharding.PositionalSharding(devices)\n",
    "pos_sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeuoOlsbO5uI"
   },
   "source": [
    "In this case, the sharding has a single positional axis, of length 8. We can use this to shard arrays whose (first) positional axis is a multiple of 8. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05EglsLbPGo0"
   },
   "outputs": [],
   "source": [
    "jax.device_put(jnp.ones(16), pos_sharding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7p19KDEPvxC"
   },
   "source": [
    "You can click the \"Sharded across 8 TPU devices\" message to show a visualization of the sharding for this array. When automatic array visualization is enabled, sharding visualizations are automatically added to any array that is sharded or replicated.\n",
    "\n",
    "We can reshape positional shardings to give them multiple axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Sr8cdyIQDni"
   },
   "outputs": [],
   "source": [
    "pos_sharding.reshape((4,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YU8KrbCQGZl"
   },
   "outputs": [],
   "source": [
    "jax.device_put(jnp.ones([8, 8]), pos_sharding.reshape((4,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8U7Tf7drQRSx"
   },
   "source": [
    "If you expand the sharding visualization above, you'll see how the two axes of the array are matched with the two axes of the sharding.\n",
    "\n",
    "You can also use shardings to indicate that certain parts of the array should be *replicated* on multiple devices, using `replicate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wiQImi8HQ2eJ"
   },
   "outputs": [],
   "source": [
    "pos_sharding.reshape((2, 4)).replicate(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSp4rCrnQ_Be"
   },
   "outputs": [],
   "source": [
    "jax.device_put(jnp.ones([8, 8]), pos_sharding.reshape((2, 4)).replicate(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytz-yk_mRD-o"
   },
   "source": [
    "Each element of an array with a replicated sharding will appear on more than one device. This is visually represented in Treescope using a multicolored pattern.\n",
    "\n",
    "You can also fully-replicate the array over all of the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFr1_cLHRDuS"
   },
   "outputs": [],
   "source": [
    "pos_sharding.replicate(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ofc7QkjDRd9O"
   },
   "outputs": [],
   "source": [
    "jax.device_put(jnp.ones([8, 8]), pos_sharding.replicate(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqvmCFxCRh_f"
   },
   "source": [
    "Fully-replicated arrays are also identified as such in the sharding summary before being expanded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzWSqgC5Qjqj"
   },
   "source": [
    "### Meshes and named shardings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iGPexZqQpS_"
   },
   "source": [
    "It is often convenient to refer to different axes of an array of devices by name instead of by position. JAX represents this using the type `jax.sharding.Mesh`. Conceptually, just as a `PositionalSharding` is essentially a positional array of devices, a `Mesh` is essentially a named array of devices, i.e. an array of devices where each axis has a name.\n",
    "\n",
    "Penzai annotates the device ID arrays of `Mesh` instances with axis names instead of axis positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MZOKGzASnlC"
   },
   "outputs": [],
   "source": [
    "mesh = jax.sharding.Mesh(devices.reshape((4, 2)), axis_names=('foo', 'bar'))\n",
    "mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsytkAohS9m5"
   },
   "source": [
    "To shard a (positionally-indexed) JAX array using a mesh, you can use `jax.sharding.NamedSharding` to assign particular axis indices to mesh axis names, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y18TEPOOTMLe"
   },
   "outputs": [],
   "source": [
    "jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('foo', 'bar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oiv6nIo4TRxW"
   },
   "outputs": [],
   "source": [
    "jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(None, ('bar', 'foo'), None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3Op7-ThTcDM"
   },
   "outputs": [],
   "source": [
    "jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec('foo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQMhXnwZTgjV"
   },
   "source": [
    "Note: Each `NamedSharding` specifies how to shard an input array's *positional axes*, since ordinary JAX arrays only have positional axes. The names in the `NamedSharding` are just a way to match the positional axes in the array with the corresponding names in the `Mesh`. For this reason, visualizations of `NamedSharding` instances are annotated with positional axes, not axis names.\n",
    "\n",
    "(Penzai already has its own mechanism for binding names to an array's positional axes: `pz.nx.NamedArray`. We'll discuss how to shard Penzai's `NamedArray` next.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPTKsWLVFMQd"
   },
   "source": [
    "## Sharding Penzai's NamedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skkgyBErYqdi"
   },
   "source": [
    "### Manually sharding NamedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMa5pJt3U7aN"
   },
   "source": [
    "Fundamentally, there are no changes when applying JAX shardings to Penzai's `NamedArray`s. Internally, a `NamedArray` is just a dataclass PyTree node that contains a JAX array and some axis name annotations, which we can see if we disable automatic array visualization temporarily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4AMqgEOVVk5"
   },
   "outputs": [],
   "source": [
    "arr = pz.nx.arange(\"foo\", 1, 4) + pz.nx.arange(\"bar\", 0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iR_e_twSVOCh"
   },
   "outputs": [],
   "source": [
    "# With automatic array visualization enabled:\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bonDVqA2Vgap"
   },
   "outputs": [],
   "source": [
    "%%autovisualize None\n",
    "# ^ With automatic array visualization disabled (and expanding it to show detail)\n",
    "pz.select(arr).at_instances_of(jax.Array).show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fh5lL50fVuJj"
   },
   "source": [
    "JAX's sharding system allows you to specify the sharding for a PyTree of arrays by using a matching PyTree of shardings. So, we can build a sharding for this named array by inserting a positional sharding into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2eQvh_AV4e4"
   },
   "outputs": [],
   "source": [
    "data_array_sharding = jax.sharding.PositionalSharding(devices).reshape((2,4)).replicate(axis=0)\n",
    "sharding_for_arr = pz.nx.NamedArray(\n",
    "    named_axes=arr.named_axes,\n",
    "    data_array=data_array_sharding,\n",
    ")\n",
    "sharding_for_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh0usyDpXDPj"
   },
   "source": [
    "Applying this sharding to the NamedArray shards the `data_array` attribute (try expanding below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADT9myFsXYCf"
   },
   "outputs": [],
   "source": [
    "%%autovisualize lambda a,p: pz.ts.ArrayAutovisualizer()(a, p) if isinstance(a, jax.Array) else None\n",
    "# (^ this line overrides the autovisualizer to show the sharding of the data array when expanded)\n",
    "\n",
    "sharded_arr = jax.device_put(arr, sharding_for_arr)\n",
    "pz.select(sharded_arr).at_instances_of(jax.Array).show_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPa-auHeYMvX"
   },
   "source": [
    "But with normal automatic array visualization, treescope will show you how the *named* axes are sharded, since that's usually what you care about when using Penzai models in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HadvQnuvYWJr"
   },
   "outputs": [],
   "source": [
    "sharded_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nlf0vWOCYuh0"
   },
   "source": [
    "### Automatically building shardings for NamedArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hy8_V9QYw6x"
   },
   "source": [
    "To simplify this process, Penzai provides some optional utilities for constructing shardings for `NamedArray` instances. These utilities take a `Mesh`, and allow you to map from `NamedArray` axis names to `Mesh` axis names across a tree of arrays.\n",
    "\n",
    "For instance, consider this tree of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBf0KIpqZoLu"
   },
   "outputs": [],
   "source": [
    "some_array_tree = {\n",
    "    \"one\": pz.nx.ones({\"a\": 4, \"b\": 8, \"c\": 6}),\n",
    "    \"two\": pz.nx.ones({\"a\": 8}),\n",
    "    \"three\": pz.nx.ones({\"b\": 4, \"d\": 12}),\n",
    "}\n",
    "some_array_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgrBL4fRaAb3"
   },
   "source": [
    "And this mesh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaDfIMbtaC7-"
   },
   "outputs": [],
   "source": [
    "mesh = jax.sharding.Mesh(devices.reshape((4, 2)), axis_names=('foo', 'bar'))\n",
    "mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIRFBn_uaHdg"
   },
   "source": [
    "We can assign each named axis in `some_array_tree` to an axis in the mesh using the `name_to_name_sharding` utility, which builds a tree of shardings that is compatible with the tree of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwKu52ifaG8K"
   },
   "outputs": [],
   "source": [
    "from penzai.toolshed import sharding_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY_DUvaAaSXu"
   },
   "outputs": [],
   "source": [
    "shardings = sharding_util.name_to_name_sharding(\n",
    "    some_array_tree,\n",
    "    mesh,\n",
    "    axis_name_to_mesh_name={\n",
    "        \"a\": \"bar\",\n",
    "        \"b\": \"foo\",\n",
    "    },\n",
    ")\n",
    "shardings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTBicKeLafVB"
   },
   "source": [
    "We can then apply those shardings to the original array tree to shard the corresponding axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAWE43sHan04"
   },
   "outputs": [],
   "source": [
    "jax.device_put(some_array_tree, shardings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xIDPTi3a-Mi"
   },
   "source": [
    "Even simpler, if you just want to call `device_put` you can bundle them into one call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Blvw8lo_bE2U"
   },
   "outputs": [],
   "source": [
    "sharding_util.name_to_name_device_put(\n",
    "    some_array_tree,\n",
    "    mesh,\n",
    "    axis_name_to_mesh_name={\n",
    "        \"a\": \"bar\",\n",
    "        \"b\": \"foo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M23P_pPCbKqW"
   },
   "source": [
    "If your mesh happens to use the exact same axis names as your arrays, you don't need the `axis_name_to_mesh_name` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjoV48jAbRLT"
   },
   "outputs": [],
   "source": [
    "already_matching_mesh = jax.sharding.Mesh(devices.reshape((4, 2)), axis_names=('b', 'a'))\n",
    "sharding_util.name_to_name_device_put(\n",
    "    some_array_tree,\n",
    "    already_matching_mesh,\n",
    "    # axis_name_to_mesh_name inferred as {\"a\":\"a\", \"b\":\"b\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORHLiG38FQ-b"
   },
   "source": [
    "## Sharding Penzai Models and Training Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYza3pi1cB8r"
   },
   "source": [
    "Penzai also provides some utilities that are specific to training and using Penzai neural newtork models. These are simple self-contained utilities that can be a good starting point, but you are free to customize them to get lower-level control when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBCz_sOQZDoJ"
   },
   "source": [
    "### Sharding Parameter Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo8KALJocV9F"
   },
   "source": [
    "Uninitialized Penzai models directly expose all of the parameter initializers to you as attributes inside your model. If you want to customize the sharding of your parameters, you can JIT-compile the initializer with the appropriate sharding, e.g.\n",
    "```python\n",
    "sharded_initializer = jax.jit(\n",
    "  pz.nn.initialize_parameters,\n",
    "  out_shardings=..., # <- insert your desired sharding specification here\n",
    ")\n",
    "params = sharded_initializer(mlp_def, jax.random.key(42))\n",
    "```\n",
    "If you want to infer `out_shardings` using the axis names of your parameters, you can do that using the helper function `initialize_parameters_sharded`. This function just traces the initializer to figure out the parameter shapes, infers the right sharding to use, and then runs your initializer accordingly.\n",
    "\n",
    "For instance, here's how you could initialize the parameters of a small transformer in a sharded way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fw_r3bbQk7do"
   },
   "outputs": [],
   "source": [
    "# Using the Gemma model architecture, but very small for demonstration purposes.\n",
    "tiny_transformer_def = gemma.model_core.GemmaTransformer.from_config(\n",
    "    gemma.model_core.GemmaTransformerConfig(\n",
    "        num_heads=2,\n",
    "        embedding_dim=64,\n",
    "        projection_dim=16,\n",
    "        single_kv_head=False,\n",
    "        mlp_hidden_dim=128,\n",
    "        num_decoder_blocks=2,\n",
    "        vocab_size=100,\n",
    "        parameter_dtype=jnp.float32,\n",
    "        activation_dtype=jnp.float32,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NOvdTWGcniz"
   },
   "outputs": [],
   "source": [
    "tiny_transformer = sharding_util.initialize_parameters_sharded(\n",
    "    tiny_transformer_def,\n",
    "    jax.random.key(42),\n",
    "    mesh=jax.sharding.Mesh(devices, axis_names=('devices',)),\n",
    "    axis_name_to_mesh_name={\n",
    "        # Shard the embedding dimension across devices.\n",
    "        \"embedding\": \"devices\",\n",
    "    },\n",
    ")\n",
    "tiny_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRJYY-p_FVNj"
   },
   "source": [
    "### Sharding Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZTvIOIgdmIX"
   },
   "source": [
    "You're encouraged to write your own custom training loop for your use case. However, the basic training step implementation in `penzai.toolshed.basic_training` does support sharded training.\n",
    "\n",
    "The easiest way to shard a training loop is to just shard your model parameters and inputs, JIT-compile the training loop, and let JAX figure out how the sharding should propagate. XLA can usually automatically infer a decent sharding for the computation and its outputs.\n",
    "\n",
    "For instance, here's how we could write a simple training loop for this tiny transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8uKXio3eiaB"
   },
   "outputs": [],
   "source": [
    "# Simple loss function for demonstration purposes.\n",
    "def simplified_xent_loss_fn(model, rng, state, input_examples):\n",
    "  del rng, state  # Unused.\n",
    "  # Run the model.\n",
    "  outputs = model(gemma.model_core.GemmaInputs.from_basic_segments(\n",
    "      input_examples[{\"seq\": pz.slice[:-1]}]\n",
    "  ))\n",
    "  # Compute log-probabilities along the \"vocabulary\" axis.\n",
    "  all_log_probs = pz.nx.nmap(jax.nn.log_softmax)(\n",
    "      outputs.untag(\"vocabulary\")\n",
    "  ).tag(\"vocabulary\")\n",
    "  # Index by the correct tokens.\n",
    "  correct_next_tokens = input_examples[{\"seq\": pz.slice[1:]}]\n",
    "  correct_log_probs = all_log_probs[{\"vocabulary\": correct_next_tokens}]\n",
    "  # Take averages.\n",
    "  loss = -correct_log_probs.untag(\"batch\", \"seq\").unwrap().mean()\n",
    "  return loss, None, {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFs07KEgl6yr"
   },
   "outputs": [],
   "source": [
    "train_step = basic_training.build_train_step_fn(\n",
    "    simplified_xent_loss_fn,\n",
    "    jit=True,\n",
    "    # donate_params_and_state=True,  # <- Uncomment to allow XLA memory optimizations.\n",
    ")\n",
    "train_state = basic_training.TrainState.initial_state(\n",
    "    model=tiny_transformer,\n",
    "    optimizer_def=optax.adamw(5e-5, weight_decay=0.01),\n",
    "    root_rng=jax.random.key(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uk9WXyEmUFh"
   },
   "outputs": [],
   "source": [
    "# Take a training step (with a dummy input in this case).\n",
    "input_examples = pz.nx.ones({\"batch\": 8, \"seq\": 20}, dtype=jnp.int32)\n",
    "updated_train_state, outs = train_step(train_state, input_examples=input_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AbOGeLHmsGK"
   },
   "outputs": [],
   "source": [
    "# Show the updated parameters.\n",
    "pz.select(updated_train_state.model).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzXwlTyrnPjl"
   },
   "source": [
    "If you inspect the parameters above, you will likely see that they are still sharded along the embedding axis (because XLA will likely infer that that keeping the same sharding is the easiest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbvdJPJpekBC"
   },
   "source": [
    "If you prefer, however, you can also manually specify what shardings you want to use, and the training step function will respect them. For instance, we can explicitly request that the model parameters and optimizer states be sharded across the \"features\" axis, and the inputs be sharded across the \"batch\" axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxIyQd48etJ7"
   },
   "outputs": [],
   "source": [
    "mesh = jax.sharding.Mesh(devices, axis_names=('devices',))\n",
    "train_step = basic_training.build_train_step_fn(\n",
    "    simplified_xent_loss_fn,\n",
    "    jit=True,\n",
    "    # Shard inputs over \"batch\" axis.\n",
    "    input_kwarg_shardings={\n",
    "        \"input_examples\": sharding_util.name_to_name_sharding(\n",
    "            input_examples,\n",
    "            mesh,\n",
    "            axis_name_to_mesh_name={\"batch\": \"devices\"},\n",
    "        ),\n",
    "    },\n",
    "    # Shard model and optimizer params over \"embedding\" axis.\n",
    "    train_state_shardings=sharding_util.name_to_name_sharding(\n",
    "        train_state,\n",
    "        mesh,\n",
    "        axis_name_to_mesh_name={\"embedding\": \"devices\"},\n",
    "        ignore_unnamed_arrays=True,\n",
    "    ),\n",
    "    # donate_params_and_state=True,  # <- Uncomment to allow XLA memory optimizations.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfNbKkx6oFUL"
   },
   "outputs": [],
   "source": [
    "# Take a training step (with a dummy input in this case).\n",
    "input_examples = pz.nx.ones({\"batch\": 8, \"seq\": 20}, dtype=jnp.int32)\n",
    "updated_train_state, outs = train_step(train_state, input_examples=input_examples)\n",
    "# Show the updated parameters.\n",
    "pz.select(updated_train_state.model).at_instances_of(pz.nn.Parameter).get_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpLaUQbCFhDM"
   },
   "source": [
    "### Adding Sharding Constraints to Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U59lObkMff19"
   },
   "source": [
    "You may want more control over the way that intermediate values are sharded. JAX allows you to control this using `jax.lax.with_sharding_constraint`, which forces a particular value to have a particular sharding.\n",
    "\n",
    "In a Penzai model, sharding constraints can be enforced by simply inserting new layers into the model at the points where you want to constrain the shardings. Penzai's `sharding_util` module provides two simple classes `ConstrainSharding` and `ConstrainShardingByName` for this purpose, defined as\n",
    "```python\n",
    "@pz.pytree_dataclass\n",
    "class ConstrainSharding(pz.Layer):\n",
    "  sharding: PyTreeOfShardings = field(metadata={\"pytree_node\": False})\n",
    "  def __call__(self, tree: Any) -> Any:\n",
    "    return jax.lax.with_sharding_constraint(tree, self.sharding)\n",
    "\n",
    "@pz.pytree_dataclass\n",
    "class ConstrainShardingByName(pz.Layer):\n",
    "  mesh: jax.sharding.Mesh = field(metadata={\"pytree_node\": False})\n",
    "  axis_name_to_mesh_name: dict[str, str | tuple[str, ...]] | None = (\n",
    "      field(default=None, metadata={\"pytree_node\": False})\n",
    "  )\n",
    "  def __call__(self, tree: PyTreeOfNamedArrays) -> PyTreeOfNamedArrays:\n",
    "    return jax.lax.with_sharding_constraint(\n",
    "        tree,\n",
    "        name_to_name_sharding(tree, self.mesh, self.axis_name_to_mesh_name),\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1fu1Z4dgUCm"
   },
   "source": [
    "You can insert them into the model using logic like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LUlJkzj-ZGcV"
   },
   "outputs": [],
   "source": [
    "# Make sure it's sharded over the batch axis after each block.\n",
    "tiny_transformer_constrained = (\n",
    "    pz.select(tiny_transformer)\n",
    "    .at_instances_of(gemma.model_core.GemmaTransformerBlock)\n",
    "    .insert_after(sharding_util.ConstrainShardingByName(\n",
    "        mesh, axis_name_to_mesh_name={\"batch\": \"devices\"}\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwxBq9TfqY7d"
   },
   "outputs": [],
   "source": [
    "# Visualize the constraints:\n",
    "pz.select(tiny_transformer_constrained).at_instances_of(sharding_util.ConstrainShardingByName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-KzB-r6gXVj"
   },
   "source": [
    "This gives you a version of the model whose intermediates will be sharded in the way you specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbTHcj0gqogd"
   },
   "outputs": [],
   "source": [
    "train_state = basic_training.TrainState.initial_state(\n",
    "    model=tiny_transformer_constrained,\n",
    "    optimizer_def=optax.adamw(5e-5, weight_decay=0.01),\n",
    "    root_rng=jax.random.key(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wCb--bUq0sF"
   },
   "outputs": [],
   "source": [
    "mesh = jax.sharding.Mesh(devices, axis_names=('devices',))\n",
    "train_step = basic_training.build_train_step_fn(\n",
    "    simplified_xent_loss_fn,\n",
    "    jit=True,\n",
    "    # Shard inputs over \"batch\" axis.\n",
    "    input_kwarg_shardings={\n",
    "        \"input_examples\": sharding_util.name_to_name_sharding(\n",
    "            input_examples,\n",
    "            mesh,\n",
    "            axis_name_to_mesh_name={\"batch\": \"devices\"},\n",
    "        ),\n",
    "    },\n",
    "    # Shard model and optimizer params over \"embedding\" axis.\n",
    "    train_state_shardings=sharding_util.name_to_name_sharding(\n",
    "        train_state,\n",
    "        mesh,\n",
    "        axis_name_to_mesh_name={\"embedding\": \"devices\"},\n",
    "        ignore_unnamed_arrays=True,\n",
    "    ),\n",
    "    # donate_params_and_state=True,  # <- Uncomment to allow XLA memory optimizations.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHhhYJT9qs9W"
   },
   "outputs": [],
   "source": [
    "# Take a training step (with a dummy input in this case).\n",
    "input_examples = pz.nx.ones({\"batch\": 8, \"seq\": 20}, dtype=jnp.int32)\n",
    "updated_train_state, outs = train_step(train_state, input_examples=input_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdRbQgtCqnAv"
   },
   "source": [
    "If you later want to change how your model's intermediates are sharded, you can simply remove these constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p_RP3QEKgrNK"
   },
   "outputs": [],
   "source": [
    "tiny_transformer_unconstrained = (\n",
    "    pz.select(tiny_transformer_constrained)\n",
    "    .at_instances_of(sharding_util.ConstrainShardingByName)\n",
    "    .remove_from_parent()\n",
    ")\n",
    "\n",
    "# No more constraints:\n",
    "(\n",
    "    pz.select(tiny_transformer_unconstrained)\n",
    "    .at_instances_of(sharding_util.ConstrainShardingByName)\n",
    "    .assert_count_is(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEqaoaNdrO_b"
   },
   "source": [
    "### Aside: Parameter checkpointing for sharded models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1lvF5lpg2q8"
   },
   "source": [
    "Note: To make it easier to save and restore parameters from checkpoints even if you've inserted sharding constraints (or made other modifications), we recommend only checkpointing the dictionary of model parameters, not the full model structure. This is how the `basic_training.TrainState` stores the parameters internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "es2dV4sXxxJV"
   },
   "outputs": [],
   "source": [
    "train_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUclsgbcyAF9"
   },
   "source": [
    "You can manually extract the parameter dictionary from a model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4KIOQqYhajC"
   },
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    param.name: param.value\n",
    "    for param in (\n",
    "        pz.select(tiny_transformer)\n",
    "        .at_instances_of(pz.nn.Parameter)\n",
    "        .get_sequence()\n",
    "    )\n",
    "}\n",
    "param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZ_b6X1xha8B"
   },
   "source": [
    "And later restore them using something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA6EARE3hyyE"
   },
   "outputs": [],
   "source": [
    "restored = (\n",
    "    pz.select(tiny_transformer)\n",
    "    .at_instances_of(pz.nn.Parameter)\n",
    "    .apply(\n",
    "        lambda param: dataclasses.replace(param, value=param_dict[param.name])\n",
    "    )\n",
    ")\n",
    "restored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJt0uNK9yFFw"
   },
   "source": [
    "If you haven't yet ininitialized your parameters, you can do something similar to initialize the `UninitializedParameter`s directly using saved values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxEG_3OyK-S"
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pz.select(tiny_transformer_def)\n",
    "    .at_instances_of(pz.nn.UninitializedParameter)\n",
    "    .apply(\n",
    "        lambda uninit: uninit.initialize_with_value(param_dict[uninit.name])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qf52VJrsrS15"
   },
   "source": [
    "You can also use this to build a PyTree with the same shape as your model parameter dictionary without initializing them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eVCaFVQhzKH"
   },
   "outputs": [],
   "source": [
    "# Produces a structure containing jax.ShapeDtypeStruct\n",
    "param_dict_structure = {\n",
    "    uninit.name: uninit.as_empty_parameter().value\n",
    "    for uninit in (\n",
    "        pz.select(tiny_transformer_def)\n",
    "        .at_instances_of(pz.nn.UninitializedParameter)\n",
    "        .get_sequence()\n",
    "    )\n",
    "}\n",
    "param_dict_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zByC5JmL0IE3"
   },
   "source": [
    "This, in turn, could be used to build a sharding specification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7QJAdSV0GWI"
   },
   "outputs": [],
   "source": [
    "param_dict_sharding = sharding_util.name_to_name_sharding(\n",
    "    param_dict_structure,\n",
    "    mesh,\n",
    "    axis_name_to_mesh_name={\"embedding\": \"devices\"},\n",
    ")\n",
    "param_dict_sharding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yr_KMf0w3A1f"
   },
   "source": [
    "You can also wrap the `NamedSharding` leaves in a `jax.ShapeDtypeStruct`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpW9hsn03FuP"
   },
   "outputs": [],
   "source": [
    "param_dict_sharding_structs = sharding_util.name_to_name_sharding(\n",
    "    param_dict_structure,\n",
    "    mesh,\n",
    "    axis_name_to_mesh_name={\"embedding\": \"devices\"},\n",
    "    as_shape_dtype_struct=True,  # <- Wraps shardings in ShapeDtypeStruct\n",
    ")\n",
    "param_dict_sharding_structs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "969v61A5yPFI"
   },
   "source": [
    "If you're using `orbax.checkpoint` for your parameters, you can configure it so that it restores the parameters directly using this sharding, using something like\n",
    "```python\n",
    "loaded_param_dict = checkpointer.restore(\n",
    "    ckpt_path,\n",
    "    args=orbax.checkpoint.args.StandardRestore(param_dict_sharding_structs),\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Jitting and Sharding Penzai Models",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
